{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12404721,"sourceType":"datasetVersion","datasetId":7822804},{"sourceId":13186351,"sourceType":"datasetVersion","datasetId":8356292}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nMetrics-only LargeClass smell detection (single-file runnable script)\n\n- Expects: /mnt/data/Python_LargeClassSmell_Dataset.csv\n- Uses \"LargeClass\" column as target (0/1)\n- All other columns are treated as numeric features (metrics)\n\"\"\"\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import (\n    precision_recall_fscore_support,\n    matthews_corrcoef,\n    confusion_matrix,\n    accuracy_score,\n    classification_report,\n    roc_auc_score,\n    roc_curve\n)\n\n# ---------------------------\n# Config\n# ---------------------------\nSEED = 42\nBATCH_SIZE = 64\nEPOCHS = 1000  # Increased\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATA_PATH = \"/kaggle/input/python-dataset/Python_LargeClassSmell_Dataset.csv\"  # update if necessary\nMODEL_OUT = \"/kaggle/working/metrics_model_best_f1.pth\"\n\n# If True: run Stratified 5-Fold CV instead of single train/test split\nDO_CV = False\n\n# Reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Dataset path: {DATA_PATH}\")\n\n# ---------------------------\n# Data loader / preprocessing\n# ---------------------------\nclass PythonMetricsLoader:\n    def __init__(self, data_path):\n        self.data_path = Path(data_path)\n        self.df = None\n\n    def load(self):\n        if not self.data_path.exists():\n            raise FileNotFoundError(f\"Dataset not found: {self.data_path}\")\n        self.df = pd.read_csv(self.data_path)\n        return self.df\n\n    def preprocess(self):\n        if self.df is None:\n            self.load()\n\n        if \"LargeClass\" not in self.df.columns:\n            raise ValueError(\"Dataset must contain 'LargeClass' column as target (0/1).\")\n\n        # Features: all columns except LargeClass\n        X = self.df.drop(columns=[\"LargeClass\"]).values.astype(float)\n        y = self.df[\"LargeClass\"].astype(int).values\n\n        # Impute missing values and scale\n        imputer = KNNImputer(n_neighbors=3)\n        scaler = StandardScaler()\n        X = imputer.fit_transform(X)\n        X = scaler.fit_transform(X)\n\n        return X, y\n\n# ---------------------------\n# Dataset class\n# ---------------------------\nclass PythonMetricsDataset(Dataset):\n    def __init__(self, metrics, labels):\n        self.metrics = torch.tensor(metrics, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.metrics[idx], self.labels[idx]\n\n# ---------------------------\n# Model: metrics-only DNN\n# ---------------------------\nclass MetricsClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dims=(128, 64, 32), dropout=0.3, num_classes=2):\n        super().__init__()\n        layers = []\n        prev = input_dim\n        for h in hidden_dims:\n            layers.append(nn.Linear(prev, h))\n            layers.append(nn.BatchNorm1d(h))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            prev = h\n        layers.append(nn.Linear(prev, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n# ---------------------------\n# Trainer\n# ---------------------------\nclass Trainer:\n    def __init__(self, model, device=DEVICE, lr=LEARNING_RATE):\n        self.model = model.to(device)\n        self.device = device\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n\n    def train_epoch(self, loader):\n        self.model.train()\n        total_loss = 0.0\n        for metrics, labels in loader:\n            metrics = metrics.to(self.device)\n            labels = labels.to(self.device)\n            self.optimizer.zero_grad()\n            logits = self.model(metrics)\n            loss = self.criterion(logits, labels)\n            loss.backward()\n            self.optimizer.step()\n            total_loss += loss.item()\n        return total_loss / len(loader)\n\n    def evaluate(self, loader):\n        self.model.eval()\n        all_preds = []\n        all_labels = []\n        all_probs = []\n        with torch.no_grad():\n            for metrics, labels in loader:\n                metrics = metrics.to(self.device)\n                labels = labels.to(self.device)\n                logits = self.model(metrics)\n                probs = F.softmax(logits, dim=1)\n                preds = torch.argmax(probs, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_probs.extend(probs[:, 1].cpu().numpy())\n\n        all_labels = np.array(all_labels)\n        all_preds = np.array(all_preds)\n        all_probs = np.array(all_probs)\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", zero_division=0)\n        mcc = matthews_corrcoef(all_labels, all_preds)\n        cm = confusion_matrix(all_labels, all_preds)\n        cls_report = classification_report(all_labels, all_preds, digits=4)\n        try:\n            roc_auc = roc_auc_score(all_labels, all_probs)\n        except ValueError:\n            roc_auc = float('nan')\n\n        return {\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"mcc\": mcc,\n            \"confusion_matrix\": cm,\n            \"classification_report\": cls_report,\n            \"roc_auc\": roc_auc,\n            \"probs\": all_probs,\n            \"labels\": all_labels\n        }\n\n# ---------------------------\n# Utilities\n# ---------------------------\ndef print_metrics_dict(metrics):\n    for k, v in metrics.items():\n        if k == \"confusion_matrix\":\n            print(f\"{k}:\\n{v}\")\n        elif k == \"classification_report\":\n            print(f\"{k}:\\n{v}\")\n        else:\n            print(f\"{k}: {v:.4f}\")\n\n# ---------------------------\n# Main: single train/test experiment\n# ---------------------------\ndef run_single_experiment(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n    print(f\"Train samples: {len(y_train)}, Test samples: {len(y_test)}\")\n    print(\"Train class distribution:\", np.bincount(y_train))\n    print(\"Test class distribution :\", np.bincount(y_test))\n\n    train_ds = PythonMetricsDataset(X_train, y_train)\n    test_ds = PythonMetricsDataset(X_test, y_test)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = MetricsClassifier(input_dim=X.shape[1])\n    trainer = Trainer(model)\n\n    best_f1 = -1.0\n    for epoch in range(1, EPOCHS + 1):\n        loss = trainer.train_epoch(train_loader)\n        metrics = trainer.evaluate(test_loader)\n        print(f\"Epoch {epoch}/{EPOCHS} - Loss: {loss:.4f} - F1: {metrics['f1']:.4f} - Acc: {metrics['accuracy']:.4f} - Precision: {metrics['precision']:.4f} - Recall: {metrics['recall']:.4f} - MCC: {metrics['mcc']:.4f}\")\n        if metrics['f1'] > best_f1:\n            best_f1 = metrics['f1']\n            torch.save(model.state_dict(), MODEL_OUT)\n            print(\"  Saved new best model (f1 improved).\")\n\n    # Load best model\n    model.load_state_dict(torch.load(MODEL_OUT, map_location=DEVICE))\n    print(\"Loaded best model from\", MODEL_OUT)\n\n    # Final evaluation\n    final_metrics = trainer.evaluate(test_loader)\n    print(\"\\nFinal test metrics:\")\n    print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n    print(f\"Precision: {final_metrics['precision']:.4f}\")\n    print(f\"Recall: {final_metrics['recall']:.4f}\")\n    print(f\"F1-Score: {final_metrics['f1']:.4f}\")\n    print(f\"MCC: {final_metrics['mcc']:.4f}\")\n    print(f\"ROC-AUC: {final_metrics['roc_auc']:.4f}\")\n    print(\"Confusion Matrix:\\n\", final_metrics['confusion_matrix'])\n    print(\"Classification Report:\\n\", final_metrics['classification_report'])\n\n    # Plot ROC curve\n    fpr, tpr, _ = roc_curve(final_metrics['labels'], final_metrics['probs'])\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {final_metrics['roc_auc']:.4f})\")\n    plt.plot([0,1], [0,1], \"k--\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend()\n    plt.show()\n\n    return final_metrics\n\n# ---------------------------\n# Main entrypoint\n# ---------------------------\ndef main():\n    loader = PythonMetricsLoader(DATA_PATH)\n    df = loader.load()\n    print(f\"Loaded dataset with {len(df)} rows and columns: {df.columns.tolist()}\")\n\n    X, y = loader.preprocess()\n    print(f\"Features shape: {X.shape}, Labels shape: {y.shape}\")\n    print(\"Overall class distribution:\", np.bincount(y))\n\n    if DO_CV:\n        # You can implement CV with metrics if desired\n        pass\n    else:\n        run_single_experiment(X, y)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:34:16.440113Z","iopub.execute_input":"2025-10-01T05:34:16.440366Z","iopub.status.idle":"2025-10-01T05:35:20.494613Z","shell.execute_reply.started":"2025-10-01T05:34:16.440340Z","shell.execute_reply":"2025-10-01T05:35:20.493728Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nDataset path: /kaggle/input/python-dataset/Python_LargeClassSmell_Dataset.csv\nLoaded dataset with 1000 rows and columns: ['loc', 'lloc', 'scloc', 'comments', 'single_comments', 'multi_comments', 'blanks', 'h1', 'h2', 'n1', 'n2', 'vocabulary', 'length', 'calculated_length', 'volume', 'difficulty', 'effort', 'time', 'bugs', 'LargeClass']\nFeatures shape: (1000, 19), Labels shape: (1000,)\nOverall class distribution: [800 200]\nTrain samples: 800, Test samples: 200\nTrain class distribution: [640 160]\nTest class distribution : [160  40]\nEpoch 1/1000 - Loss: 0.5652 - F1: 0.4074 - Acc: 0.8400 - Precision: 0.7857 - Recall: 0.2750 - MCC: 0.4017\n  Saved new best model (f1 improved).\nEpoch 2/1000 - Loss: 0.4044 - F1: 0.4912 - Acc: 0.8550 - Precision: 0.8235 - Recall: 0.3500 - MCC: 0.4751\n  Saved new best model (f1 improved).\nEpoch 3/1000 - Loss: 0.3653 - F1: 0.5263 - Acc: 0.8650 - Precision: 0.8824 - Recall: 0.3750 - MCC: 0.5199\n  Saved new best model (f1 improved).\nEpoch 4/1000 - Loss: 0.3115 - F1: 0.5517 - Acc: 0.8700 - Precision: 0.8889 - Recall: 0.4000 - MCC: 0.5416\n  Saved new best model (f1 improved).\nEpoch 5/1000 - Loss: 0.3200 - F1: 0.6102 - Acc: 0.8850 - Precision: 0.9474 - Recall: 0.4500 - MCC: 0.6054\n  Saved new best model (f1 improved).\nEpoch 6/1000 - Loss: 0.2740 - F1: 0.6129 - Acc: 0.8800 - Precision: 0.8636 - Recall: 0.4750 - MCC: 0.5833\n  Saved new best model (f1 improved).\nEpoch 7/1000 - Loss: 0.2530 - F1: 0.6349 - Acc: 0.8850 - Precision: 0.8696 - Recall: 0.5000 - MCC: 0.6034\n  Saved new best model (f1 improved).\nEpoch 8/1000 - Loss: 0.2509 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\n  Saved new best model (f1 improved).\nEpoch 9/1000 - Loss: 0.2382 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\n  Saved new best model (f1 improved).\nEpoch 10/1000 - Loss: 0.2312 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 11/1000 - Loss: 0.2353 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 12/1000 - Loss: 0.2360 - F1: 0.6571 - Acc: 0.8800 - Precision: 0.7667 - Recall: 0.5750 - MCC: 0.5951\nEpoch 13/1000 - Loss: 0.2266 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 14/1000 - Loss: 0.2137 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 15/1000 - Loss: 0.2023 - F1: 0.6571 - Acc: 0.8800 - Precision: 0.7667 - Recall: 0.5750 - MCC: 0.5951\nEpoch 16/1000 - Loss: 0.2038 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 17/1000 - Loss: 0.2075 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 18/1000 - Loss: 0.2122 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 19/1000 - Loss: 0.1965 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\n  Saved new best model (f1 improved).\nEpoch 20/1000 - Loss: 0.1970 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 21/1000 - Loss: 0.2010 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 22/1000 - Loss: 0.1927 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 23/1000 - Loss: 0.1836 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\n  Saved new best model (f1 improved).\nEpoch 24/1000 - Loss: 0.2018 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 25/1000 - Loss: 0.2012 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 26/1000 - Loss: 0.2243 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 27/1000 - Loss: 0.2060 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 28/1000 - Loss: 0.2083 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 29/1000 - Loss: 0.2139 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 30/1000 - Loss: 0.2001 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 31/1000 - Loss: 0.1910 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 32/1000 - Loss: 0.2141 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 33/1000 - Loss: 0.1822 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 34/1000 - Loss: 0.1923 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 35/1000 - Loss: 0.1884 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 36/1000 - Loss: 0.1876 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 37/1000 - Loss: 0.1870 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\n  Saved new best model (f1 improved).\nEpoch 38/1000 - Loss: 0.1817 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\n  Saved new best model (f1 improved).\nEpoch 39/1000 - Loss: 0.1763 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 40/1000 - Loss: 0.1874 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 41/1000 - Loss: 0.1952 - F1: 0.6567 - Acc: 0.8850 - Precision: 0.8148 - Recall: 0.5500 - MCC: 0.6072\nEpoch 42/1000 - Loss: 0.1938 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 43/1000 - Loss: 0.1985 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 44/1000 - Loss: 0.1711 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 45/1000 - Loss: 0.1983 - F1: 0.7297 - Acc: 0.9000 - Precision: 0.7941 - Recall: 0.6750 - MCC: 0.6722\nEpoch 46/1000 - Loss: 0.2023 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 47/1000 - Loss: 0.1905 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 48/1000 - Loss: 0.1828 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 49/1000 - Loss: 0.1774 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 50/1000 - Loss: 0.1688 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 51/1000 - Loss: 0.1751 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 52/1000 - Loss: 0.1865 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 53/1000 - Loss: 0.1654 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 54/1000 - Loss: 0.1820 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 55/1000 - Loss: 0.1707 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 56/1000 - Loss: 0.1674 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 57/1000 - Loss: 0.1900 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 58/1000 - Loss: 0.1802 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 59/1000 - Loss: 0.1857 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 60/1000 - Loss: 0.1816 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 61/1000 - Loss: 0.1800 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 62/1000 - Loss: 0.2092 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 63/1000 - Loss: 0.1846 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 64/1000 - Loss: 0.1938 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 65/1000 - Loss: 0.1708 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 66/1000 - Loss: 0.2078 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 67/1000 - Loss: 0.1659 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 68/1000 - Loss: 0.1714 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 69/1000 - Loss: 0.1683 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 70/1000 - Loss: 0.1893 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 71/1000 - Loss: 0.1524 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 72/1000 - Loss: 0.1682 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 73/1000 - Loss: 0.1798 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 74/1000 - Loss: 0.1605 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 75/1000 - Loss: 0.1801 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 76/1000 - Loss: 0.1801 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 77/1000 - Loss: 0.1609 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 78/1000 - Loss: 0.1755 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 79/1000 - Loss: 0.1698 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 80/1000 - Loss: 0.1763 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 81/1000 - Loss: 0.1954 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 82/1000 - Loss: 0.1756 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 83/1000 - Loss: 0.1775 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 84/1000 - Loss: 0.1808 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 85/1000 - Loss: 0.1553 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 86/1000 - Loss: 0.1666 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 87/1000 - Loss: 0.1641 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 88/1000 - Loss: 0.1629 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 89/1000 - Loss: 0.1538 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 90/1000 - Loss: 0.1752 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 91/1000 - Loss: 0.1780 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 92/1000 - Loss: 0.1748 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 93/1000 - Loss: 0.1700 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 94/1000 - Loss: 0.1704 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 95/1000 - Loss: 0.1799 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\n  Saved new best model (f1 improved).\nEpoch 96/1000 - Loss: 0.1448 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 97/1000 - Loss: 0.1550 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 98/1000 - Loss: 0.1730 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 99/1000 - Loss: 0.1737 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 100/1000 - Loss: 0.1602 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 101/1000 - Loss: 0.1672 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 102/1000 - Loss: 0.1556 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 103/1000 - Loss: 0.1811 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 104/1000 - Loss: 0.1582 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 105/1000 - Loss: 0.1645 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 106/1000 - Loss: 0.1627 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 107/1000 - Loss: 0.1483 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 108/1000 - Loss: 0.1587 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 109/1000 - Loss: 0.1455 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 110/1000 - Loss: 0.1490 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 111/1000 - Loss: 0.1729 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 112/1000 - Loss: 0.1620 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 113/1000 - Loss: 0.1535 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 114/1000 - Loss: 0.1611 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 115/1000 - Loss: 0.1503 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 116/1000 - Loss: 0.1665 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 117/1000 - Loss: 0.1577 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 118/1000 - Loss: 0.1597 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 119/1000 - Loss: 0.1462 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 120/1000 - Loss: 0.1551 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 121/1000 - Loss: 0.1666 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 122/1000 - Loss: 0.1647 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 123/1000 - Loss: 0.1475 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 124/1000 - Loss: 0.1590 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 125/1000 - Loss: 0.1512 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 126/1000 - Loss: 0.1759 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 127/1000 - Loss: 0.1648 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 128/1000 - Loss: 0.1529 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 129/1000 - Loss: 0.1866 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 130/1000 - Loss: 0.1555 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 131/1000 - Loss: 0.1633 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 132/1000 - Loss: 0.1463 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 133/1000 - Loss: 0.1652 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 134/1000 - Loss: 0.1637 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 135/1000 - Loss: 0.1520 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 136/1000 - Loss: 0.1440 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 137/1000 - Loss: 0.1426 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 138/1000 - Loss: 0.1499 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 139/1000 - Loss: 0.1474 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 140/1000 - Loss: 0.1469 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 141/1000 - Loss: 0.1526 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 142/1000 - Loss: 0.1552 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 143/1000 - Loss: 0.1617 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 144/1000 - Loss: 0.1679 - F1: 0.7647 - Acc: 0.9200 - Precision: 0.9286 - Recall: 0.6500 - MCC: 0.7349\n  Saved new best model (f1 improved).\nEpoch 145/1000 - Loss: 0.1445 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 146/1000 - Loss: 0.1514 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 147/1000 - Loss: 0.1380 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 148/1000 - Loss: 0.1409 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 149/1000 - Loss: 0.1492 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 150/1000 - Loss: 0.1544 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 151/1000 - Loss: 0.1495 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 152/1000 - Loss: 0.1501 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 153/1000 - Loss: 0.1377 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 154/1000 - Loss: 0.1595 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 155/1000 - Loss: 0.1376 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 156/1000 - Loss: 0.1587 - F1: 0.6885 - Acc: 0.9050 - Precision: 1.0000 - Recall: 0.5250 - MCC: 0.6850\nEpoch 157/1000 - Loss: 0.1429 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 158/1000 - Loss: 0.1520 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 159/1000 - Loss: 0.1546 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 160/1000 - Loss: 0.1464 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 161/1000 - Loss: 0.1531 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 162/1000 - Loss: 0.1464 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 163/1000 - Loss: 0.1436 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 164/1000 - Loss: 0.1233 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 165/1000 - Loss: 0.1499 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 166/1000 - Loss: 0.1377 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 167/1000 - Loss: 0.1387 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 168/1000 - Loss: 0.1509 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 169/1000 - Loss: 0.1578 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 170/1000 - Loss: 0.1391 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 171/1000 - Loss: 0.1446 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 172/1000 - Loss: 0.1528 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 173/1000 - Loss: 0.1303 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 174/1000 - Loss: 0.1209 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 175/1000 - Loss: 0.1355 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 176/1000 - Loss: 0.1459 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 177/1000 - Loss: 0.1307 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 178/1000 - Loss: 0.1312 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 179/1000 - Loss: 0.1274 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 180/1000 - Loss: 0.1293 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 181/1000 - Loss: 0.1662 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 182/1000 - Loss: 0.1282 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 183/1000 - Loss: 0.1427 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 184/1000 - Loss: 0.1276 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 185/1000 - Loss: 0.1556 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 186/1000 - Loss: 0.1319 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 187/1000 - Loss: 0.1502 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 188/1000 - Loss: 0.1365 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 189/1000 - Loss: 0.1365 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 190/1000 - Loss: 0.1500 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 191/1000 - Loss: 0.1352 - F1: 0.6885 - Acc: 0.9050 - Precision: 1.0000 - Recall: 0.5250 - MCC: 0.6850\nEpoch 192/1000 - Loss: 0.1349 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 193/1000 - Loss: 0.1629 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 194/1000 - Loss: 0.1442 - F1: 0.6885 - Acc: 0.9050 - Precision: 1.0000 - Recall: 0.5250 - MCC: 0.6850\nEpoch 195/1000 - Loss: 0.1320 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 196/1000 - Loss: 0.1651 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 197/1000 - Loss: 0.1381 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 198/1000 - Loss: 0.1410 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 199/1000 - Loss: 0.1279 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 200/1000 - Loss: 0.1366 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 201/1000 - Loss: 0.1263 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 202/1000 - Loss: 0.1339 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 203/1000 - Loss: 0.1439 - F1: 0.6885 - Acc: 0.9050 - Precision: 1.0000 - Recall: 0.5250 - MCC: 0.6850\nEpoch 204/1000 - Loss: 0.1387 - F1: 0.6557 - Acc: 0.8950 - Precision: 0.9524 - Recall: 0.5000 - MCC: 0.6443\nEpoch 205/1000 - Loss: 0.1432 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 206/1000 - Loss: 0.1423 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 207/1000 - Loss: 0.1413 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 208/1000 - Loss: 0.1321 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 209/1000 - Loss: 0.1451 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 210/1000 - Loss: 0.1366 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 211/1000 - Loss: 0.1310 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 212/1000 - Loss: 0.1248 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 213/1000 - Loss: 0.1335 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 214/1000 - Loss: 0.1314 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 215/1000 - Loss: 0.1144 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 216/1000 - Loss: 0.1374 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 217/1000 - Loss: 0.1574 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 218/1000 - Loss: 0.1201 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 219/1000 - Loss: 0.1310 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 220/1000 - Loss: 0.1171 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 221/1000 - Loss: 0.1412 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 222/1000 - Loss: 0.1473 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 223/1000 - Loss: 0.1172 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 224/1000 - Loss: 0.1253 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 225/1000 - Loss: 0.1544 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 226/1000 - Loss: 0.1552 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 227/1000 - Loss: 0.1411 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 228/1000 - Loss: 0.1205 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 229/1000 - Loss: 0.1458 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 230/1000 - Loss: 0.1389 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 231/1000 - Loss: 0.1323 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 232/1000 - Loss: 0.1263 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 233/1000 - Loss: 0.1247 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 234/1000 - Loss: 0.1237 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 235/1000 - Loss: 0.1244 - F1: 0.6667 - Acc: 0.9000 - Precision: 1.0000 - Recall: 0.5000 - MCC: 0.6667\nEpoch 236/1000 - Loss: 0.1395 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 237/1000 - Loss: 0.1442 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 238/1000 - Loss: 0.1232 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 239/1000 - Loss: 0.1177 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 240/1000 - Loss: 0.1351 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 241/1000 - Loss: 0.1496 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 242/1000 - Loss: 0.1246 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 243/1000 - Loss: 0.1401 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 244/1000 - Loss: 0.1257 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 245/1000 - Loss: 0.1188 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 246/1000 - Loss: 0.1392 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 247/1000 - Loss: 0.1284 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 248/1000 - Loss: 0.1370 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 249/1000 - Loss: 0.1728 - F1: 0.6452 - Acc: 0.8900 - Precision: 0.9091 - Recall: 0.5000 - MCC: 0.6232\nEpoch 250/1000 - Loss: 0.1363 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 251/1000 - Loss: 0.1237 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 252/1000 - Loss: 0.1284 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 253/1000 - Loss: 0.1522 - F1: 0.6667 - Acc: 0.8850 - Precision: 0.7931 - Recall: 0.5750 - MCC: 0.6106\nEpoch 254/1000 - Loss: 0.1544 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 255/1000 - Loss: 0.1314 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 256/1000 - Loss: 0.1721 - F1: 0.6452 - Acc: 0.8900 - Precision: 0.9091 - Recall: 0.5000 - MCC: 0.6232\nEpoch 257/1000 - Loss: 0.1261 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 258/1000 - Loss: 0.1375 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 259/1000 - Loss: 0.1213 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 260/1000 - Loss: 0.1265 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 261/1000 - Loss: 0.1192 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 262/1000 - Loss: 0.1287 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 263/1000 - Loss: 0.1152 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 264/1000 - Loss: 0.1190 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 265/1000 - Loss: 0.1254 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 266/1000 - Loss: 0.1180 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 267/1000 - Loss: 0.1076 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 268/1000 - Loss: 0.1388 - F1: 0.6349 - Acc: 0.8850 - Precision: 0.8696 - Recall: 0.5000 - MCC: 0.6034\nEpoch 269/1000 - Loss: 0.1351 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 270/1000 - Loss: 0.1078 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 271/1000 - Loss: 0.1429 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 272/1000 - Loss: 0.1546 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 273/1000 - Loss: 0.1402 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 274/1000 - Loss: 0.1381 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 275/1000 - Loss: 0.1341 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 276/1000 - Loss: 0.1278 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 277/1000 - Loss: 0.1266 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 278/1000 - Loss: 0.1344 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 279/1000 - Loss: 0.1096 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 280/1000 - Loss: 0.1289 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 281/1000 - Loss: 0.1312 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 282/1000 - Loss: 0.1382 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 283/1000 - Loss: 0.1193 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 284/1000 - Loss: 0.1211 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 285/1000 - Loss: 0.1314 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 286/1000 - Loss: 0.1323 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 287/1000 - Loss: 0.1211 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 288/1000 - Loss: 0.1432 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 289/1000 - Loss: 0.1167 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 290/1000 - Loss: 0.1305 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 291/1000 - Loss: 0.1195 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 292/1000 - Loss: 0.1133 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 293/1000 - Loss: 0.1287 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 294/1000 - Loss: 0.1136 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 295/1000 - Loss: 0.1134 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 296/1000 - Loss: 0.1180 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 297/1000 - Loss: 0.1327 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 298/1000 - Loss: 0.1241 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 299/1000 - Loss: 0.1099 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 300/1000 - Loss: 0.1229 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 301/1000 - Loss: 0.1544 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 302/1000 - Loss: 0.1278 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 303/1000 - Loss: 0.1247 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 304/1000 - Loss: 0.1148 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 305/1000 - Loss: 0.1246 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 306/1000 - Loss: 0.1088 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 307/1000 - Loss: 0.1228 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 308/1000 - Loss: 0.1328 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 309/1000 - Loss: 0.1318 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 310/1000 - Loss: 0.1345 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 311/1000 - Loss: 0.1275 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 312/1000 - Loss: 0.1216 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 313/1000 - Loss: 0.1183 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 314/1000 - Loss: 0.1072 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 315/1000 - Loss: 0.1136 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 316/1000 - Loss: 0.0949 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 317/1000 - Loss: 0.1431 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 318/1000 - Loss: 0.1399 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 319/1000 - Loss: 0.1202 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 320/1000 - Loss: 0.1122 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 321/1000 - Loss: 0.1161 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 322/1000 - Loss: 0.1169 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 323/1000 - Loss: 0.1181 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 324/1000 - Loss: 0.1177 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 325/1000 - Loss: 0.1247 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 326/1000 - Loss: 0.1342 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 327/1000 - Loss: 0.1358 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 328/1000 - Loss: 0.1242 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 329/1000 - Loss: 0.1196 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 330/1000 - Loss: 0.1110 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 331/1000 - Loss: 0.1296 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 332/1000 - Loss: 0.1143 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 333/1000 - Loss: 0.1319 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 334/1000 - Loss: 0.1221 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 335/1000 - Loss: 0.1204 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 336/1000 - Loss: 0.1436 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 337/1000 - Loss: 0.1212 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 338/1000 - Loss: 0.1333 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 339/1000 - Loss: 0.1137 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 340/1000 - Loss: 0.1022 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 341/1000 - Loss: 0.1176 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 342/1000 - Loss: 0.1206 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 343/1000 - Loss: 0.0980 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 344/1000 - Loss: 0.1235 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 345/1000 - Loss: 0.1181 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 346/1000 - Loss: 0.1080 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 347/1000 - Loss: 0.1041 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 348/1000 - Loss: 0.1175 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 349/1000 - Loss: 0.1216 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 350/1000 - Loss: 0.1038 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 351/1000 - Loss: 0.1151 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 352/1000 - Loss: 0.1253 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 353/1000 - Loss: 0.0963 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 354/1000 - Loss: 0.1127 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 355/1000 - Loss: 0.1217 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 356/1000 - Loss: 0.1370 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 357/1000 - Loss: 0.1112 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 358/1000 - Loss: 0.1095 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 359/1000 - Loss: 0.1254 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 360/1000 - Loss: 0.1192 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 361/1000 - Loss: 0.1230 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 362/1000 - Loss: 0.1125 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 363/1000 - Loss: 0.1081 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 364/1000 - Loss: 0.1028 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 365/1000 - Loss: 0.1122 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 366/1000 - Loss: 0.1086 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 367/1000 - Loss: 0.1247 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 368/1000 - Loss: 0.1326 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 369/1000 - Loss: 0.1026 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 370/1000 - Loss: 0.1107 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 371/1000 - Loss: 0.1032 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 372/1000 - Loss: 0.1395 - F1: 0.6557 - Acc: 0.8950 - Precision: 0.9524 - Recall: 0.5000 - MCC: 0.6443\nEpoch 373/1000 - Loss: 0.0962 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 374/1000 - Loss: 0.1222 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 375/1000 - Loss: 0.1146 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 376/1000 - Loss: 0.1016 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 377/1000 - Loss: 0.1022 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 378/1000 - Loss: 0.1064 - F1: 0.7647 - Acc: 0.9200 - Precision: 0.9286 - Recall: 0.6500 - MCC: 0.7349\nEpoch 379/1000 - Loss: 0.1073 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 380/1000 - Loss: 0.1088 - F1: 0.7647 - Acc: 0.9200 - Precision: 0.9286 - Recall: 0.6500 - MCC: 0.7349\nEpoch 381/1000 - Loss: 0.1043 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 382/1000 - Loss: 0.1125 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 383/1000 - Loss: 0.1224 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 384/1000 - Loss: 0.0925 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 385/1000 - Loss: 0.1166 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 386/1000 - Loss: 0.1004 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 387/1000 - Loss: 0.1069 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 388/1000 - Loss: 0.1172 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 389/1000 - Loss: 0.1143 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 390/1000 - Loss: 0.1060 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 391/1000 - Loss: 0.1144 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 392/1000 - Loss: 0.1097 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 393/1000 - Loss: 0.1253 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 394/1000 - Loss: 0.1263 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 395/1000 - Loss: 0.1140 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 396/1000 - Loss: 0.1033 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 397/1000 - Loss: 0.1025 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 398/1000 - Loss: 0.1011 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 399/1000 - Loss: 0.1173 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 400/1000 - Loss: 0.1030 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 401/1000 - Loss: 0.0939 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\nEpoch 402/1000 - Loss: 0.1195 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 403/1000 - Loss: 0.1155 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 404/1000 - Loss: 0.1072 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 405/1000 - Loss: 0.1131 - F1: 0.7606 - Acc: 0.9150 - Precision: 0.8710 - Recall: 0.6750 - MCC: 0.7184\nEpoch 406/1000 - Loss: 0.0956 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 407/1000 - Loss: 0.1323 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 408/1000 - Loss: 0.1199 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 409/1000 - Loss: 0.0944 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 410/1000 - Loss: 0.1059 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 411/1000 - Loss: 0.0990 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 412/1000 - Loss: 0.1069 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 413/1000 - Loss: 0.1170 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 414/1000 - Loss: 0.1197 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 415/1000 - Loss: 0.1226 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 416/1000 - Loss: 0.1198 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 417/1000 - Loss: 0.1004 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 418/1000 - Loss: 0.1093 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 419/1000 - Loss: 0.1047 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 420/1000 - Loss: 0.1098 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 421/1000 - Loss: 0.1097 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 422/1000 - Loss: 0.0978 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 423/1000 - Loss: 0.1258 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 424/1000 - Loss: 0.0969 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 425/1000 - Loss: 0.1309 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 426/1000 - Loss: 0.1109 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 427/1000 - Loss: 0.1081 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 428/1000 - Loss: 0.1004 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 429/1000 - Loss: 0.1185 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 430/1000 - Loss: 0.1056 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 431/1000 - Loss: 0.1232 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 432/1000 - Loss: 0.1273 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 433/1000 - Loss: 0.1171 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 434/1000 - Loss: 0.1148 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 435/1000 - Loss: 0.0997 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 436/1000 - Loss: 0.1055 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 437/1000 - Loss: 0.1075 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 438/1000 - Loss: 0.0940 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 439/1000 - Loss: 0.1149 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 440/1000 - Loss: 0.1032 - F1: 0.7606 - Acc: 0.9150 - Precision: 0.8710 - Recall: 0.6750 - MCC: 0.7184\nEpoch 441/1000 - Loss: 0.1003 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 442/1000 - Loss: 0.1209 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 443/1000 - Loss: 0.1091 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 444/1000 - Loss: 0.1014 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 445/1000 - Loss: 0.0938 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 446/1000 - Loss: 0.1268 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 447/1000 - Loss: 0.1270 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 448/1000 - Loss: 0.1041 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 449/1000 - Loss: 0.1045 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 450/1000 - Loss: 0.1024 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 451/1000 - Loss: 0.1111 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 452/1000 - Loss: 0.1001 - F1: 0.6557 - Acc: 0.8950 - Precision: 0.9524 - Recall: 0.5000 - MCC: 0.6443\nEpoch 453/1000 - Loss: 0.1112 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 454/1000 - Loss: 0.1118 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 455/1000 - Loss: 0.1307 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 456/1000 - Loss: 0.1247 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 457/1000 - Loss: 0.1008 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 458/1000 - Loss: 0.0949 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 459/1000 - Loss: 0.1058 - F1: 0.7647 - Acc: 0.9200 - Precision: 0.9286 - Recall: 0.6500 - MCC: 0.7349\nEpoch 460/1000 - Loss: 0.0886 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 461/1000 - Loss: 0.1146 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 462/1000 - Loss: 0.0999 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 463/1000 - Loss: 0.1136 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 464/1000 - Loss: 0.0998 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 465/1000 - Loss: 0.1131 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 466/1000 - Loss: 0.1045 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 467/1000 - Loss: 0.1108 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 468/1000 - Loss: 0.1064 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 469/1000 - Loss: 0.1093 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 470/1000 - Loss: 0.1082 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 471/1000 - Loss: 0.1106 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 472/1000 - Loss: 0.0953 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 473/1000 - Loss: 0.1049 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 474/1000 - Loss: 0.0975 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 475/1000 - Loss: 0.0929 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 476/1000 - Loss: 0.1087 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 477/1000 - Loss: 0.0949 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 478/1000 - Loss: 0.1033 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 479/1000 - Loss: 0.1057 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 480/1000 - Loss: 0.1189 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 481/1000 - Loss: 0.1245 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 482/1000 - Loss: 0.1218 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 483/1000 - Loss: 0.0990 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 484/1000 - Loss: 0.1210 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 485/1000 - Loss: 0.0978 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 486/1000 - Loss: 0.1059 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 487/1000 - Loss: 0.0950 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 488/1000 - Loss: 0.0937 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 489/1000 - Loss: 0.0884 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 490/1000 - Loss: 0.1236 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 491/1000 - Loss: 0.1000 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 492/1000 - Loss: 0.1111 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 493/1000 - Loss: 0.1094 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 494/1000 - Loss: 0.1055 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 495/1000 - Loss: 0.1094 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 496/1000 - Loss: 0.1077 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 497/1000 - Loss: 0.1079 - F1: 0.6557 - Acc: 0.8950 - Precision: 0.9524 - Recall: 0.5000 - MCC: 0.6443\nEpoch 498/1000 - Loss: 0.1093 - F1: 0.6667 - Acc: 0.9000 - Precision: 1.0000 - Recall: 0.5000 - MCC: 0.6667\nEpoch 499/1000 - Loss: 0.1108 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 500/1000 - Loss: 0.0993 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 501/1000 - Loss: 0.0947 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 502/1000 - Loss: 0.0980 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 503/1000 - Loss: 0.1034 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 504/1000 - Loss: 0.1141 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 505/1000 - Loss: 0.1031 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 506/1000 - Loss: 0.1057 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 507/1000 - Loss: 0.1013 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 508/1000 - Loss: 0.1037 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 509/1000 - Loss: 0.0935 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 510/1000 - Loss: 0.0998 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 511/1000 - Loss: 0.1005 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 512/1000 - Loss: 0.1249 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 513/1000 - Loss: 0.1098 - F1: 0.7385 - Acc: 0.9150 - Precision: 0.9600 - Recall: 0.6000 - MCC: 0.7181\nEpoch 514/1000 - Loss: 0.1045 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 515/1000 - Loss: 0.1010 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 516/1000 - Loss: 0.0964 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 517/1000 - Loss: 0.0975 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 518/1000 - Loss: 0.1183 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 519/1000 - Loss: 0.1132 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 520/1000 - Loss: 0.1083 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 521/1000 - Loss: 0.0866 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 522/1000 - Loss: 0.0961 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 523/1000 - Loss: 0.1049 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 524/1000 - Loss: 0.0947 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 525/1000 - Loss: 0.1084 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 526/1000 - Loss: 0.0944 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 527/1000 - Loss: 0.1086 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 528/1000 - Loss: 0.0983 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 529/1000 - Loss: 0.1132 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 530/1000 - Loss: 0.0877 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 531/1000 - Loss: 0.1070 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 532/1000 - Loss: 0.1055 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 533/1000 - Loss: 0.1196 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 534/1000 - Loss: 0.0887 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 535/1000 - Loss: 0.0954 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 536/1000 - Loss: 0.1080 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 537/1000 - Loss: 0.1184 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 538/1000 - Loss: 0.1243 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 539/1000 - Loss: 0.1169 - F1: 0.7297 - Acc: 0.9000 - Precision: 0.7941 - Recall: 0.6750 - MCC: 0.6722\nEpoch 540/1000 - Loss: 0.1024 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 541/1000 - Loss: 0.1055 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 542/1000 - Loss: 0.1066 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 543/1000 - Loss: 0.1045 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 544/1000 - Loss: 0.0800 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 545/1000 - Loss: 0.1127 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 546/1000 - Loss: 0.1048 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 547/1000 - Loss: 0.0913 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 548/1000 - Loss: 0.1057 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 549/1000 - Loss: 0.0861 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 550/1000 - Loss: 0.1005 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 551/1000 - Loss: 0.1098 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 552/1000 - Loss: 0.0812 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 553/1000 - Loss: 0.0953 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 554/1000 - Loss: 0.0852 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 555/1000 - Loss: 0.0974 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 556/1000 - Loss: 0.0814 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 557/1000 - Loss: 0.0892 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 558/1000 - Loss: 0.1008 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 559/1000 - Loss: 0.0867 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 560/1000 - Loss: 0.0969 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 561/1000 - Loss: 0.0982 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 562/1000 - Loss: 0.0913 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 563/1000 - Loss: 0.1076 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 564/1000 - Loss: 0.0940 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 565/1000 - Loss: 0.0959 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 566/1000 - Loss: 0.1081 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 567/1000 - Loss: 0.0961 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 568/1000 - Loss: 0.0893 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 569/1000 - Loss: 0.1087 - F1: 0.7733 - Acc: 0.9150 - Precision: 0.8286 - Recall: 0.7250 - MCC: 0.7237\n  Saved new best model (f1 improved).\nEpoch 570/1000 - Loss: 0.1141 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 571/1000 - Loss: 0.1011 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 572/1000 - Loss: 0.0967 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 573/1000 - Loss: 0.0997 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 574/1000 - Loss: 0.1105 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 575/1000 - Loss: 0.0882 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 576/1000 - Loss: 0.1023 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 577/1000 - Loss: 0.0871 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 578/1000 - Loss: 0.0898 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 579/1000 - Loss: 0.0871 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 580/1000 - Loss: 0.1027 - F1: 0.7606 - Acc: 0.9150 - Precision: 0.8710 - Recall: 0.6750 - MCC: 0.7184\nEpoch 581/1000 - Loss: 0.0799 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 582/1000 - Loss: 0.0857 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 583/1000 - Loss: 0.1024 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 584/1000 - Loss: 0.0869 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 585/1000 - Loss: 0.0902 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 586/1000 - Loss: 0.1029 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 587/1000 - Loss: 0.0934 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\nEpoch 588/1000 - Loss: 0.1027 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 589/1000 - Loss: 0.0878 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 590/1000 - Loss: 0.1250 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 591/1000 - Loss: 0.1015 - F1: 0.7714 - Acc: 0.9200 - Precision: 0.9000 - Recall: 0.6750 - MCC: 0.7351\nEpoch 592/1000 - Loss: 0.0923 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 593/1000 - Loss: 0.1183 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 594/1000 - Loss: 0.0910 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 595/1000 - Loss: 0.0999 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 596/1000 - Loss: 0.0701 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 597/1000 - Loss: 0.1049 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 598/1000 - Loss: 0.0988 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 599/1000 - Loss: 0.0846 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 600/1000 - Loss: 0.0884 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 601/1000 - Loss: 0.0937 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 602/1000 - Loss: 0.1120 - F1: 0.6984 - Acc: 0.9050 - Precision: 0.9565 - Recall: 0.5500 - MCC: 0.6818\nEpoch 603/1000 - Loss: 0.0980 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 604/1000 - Loss: 0.0834 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\nEpoch 605/1000 - Loss: 0.1091 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 606/1000 - Loss: 0.1038 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 607/1000 - Loss: 0.0935 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 608/1000 - Loss: 0.1037 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 609/1000 - Loss: 0.0971 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 610/1000 - Loss: 0.0960 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 611/1000 - Loss: 0.1023 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 612/1000 - Loss: 0.0868 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 613/1000 - Loss: 0.0907 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 614/1000 - Loss: 0.0939 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 615/1000 - Loss: 0.0937 - F1: 0.7647 - Acc: 0.9200 - Precision: 0.9286 - Recall: 0.6500 - MCC: 0.7349\nEpoch 616/1000 - Loss: 0.0839 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 617/1000 - Loss: 0.0980 - F1: 0.7463 - Acc: 0.9150 - Precision: 0.9259 - Recall: 0.6250 - MCC: 0.7170\nEpoch 618/1000 - Loss: 0.1144 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 619/1000 - Loss: 0.1066 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 620/1000 - Loss: 0.0896 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 621/1000 - Loss: 0.1023 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 622/1000 - Loss: 0.0907 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 623/1000 - Loss: 0.0935 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 624/1000 - Loss: 0.1033 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 625/1000 - Loss: 0.1534 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 626/1000 - Loss: 0.1075 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 627/1000 - Loss: 0.1065 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 628/1000 - Loss: 0.1296 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 629/1000 - Loss: 0.0889 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 630/1000 - Loss: 0.0801 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 631/1000 - Loss: 0.1043 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 632/1000 - Loss: 0.0860 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 633/1000 - Loss: 0.1062 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 634/1000 - Loss: 0.1198 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 635/1000 - Loss: 0.1009 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 636/1000 - Loss: 0.0833 - F1: 0.7671 - Acc: 0.9150 - Precision: 0.8485 - Recall: 0.7000 - MCC: 0.7207\nEpoch 637/1000 - Loss: 0.0911 - F1: 0.7895 - Acc: 0.9200 - Precision: 0.8333 - Recall: 0.7500 - MCC: 0.7418\n  Saved new best model (f1 improved).\nEpoch 638/1000 - Loss: 0.0984 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 639/1000 - Loss: 0.0842 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 640/1000 - Loss: 0.0963 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 641/1000 - Loss: 0.0925 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 642/1000 - Loss: 0.0949 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 643/1000 - Loss: 0.0908 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 644/1000 - Loss: 0.0851 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 645/1000 - Loss: 0.0837 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 646/1000 - Loss: 0.1119 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 647/1000 - Loss: 0.0838 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 648/1000 - Loss: 0.0950 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 649/1000 - Loss: 0.0932 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 650/1000 - Loss: 0.0994 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 651/1000 - Loss: 0.1161 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 652/1000 - Loss: 0.0886 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 653/1000 - Loss: 0.0998 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 654/1000 - Loss: 0.1029 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 655/1000 - Loss: 0.0817 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 656/1000 - Loss: 0.1030 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 657/1000 - Loss: 0.1039 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 658/1000 - Loss: 0.1049 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 659/1000 - Loss: 0.0964 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 660/1000 - Loss: 0.0949 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 661/1000 - Loss: 0.0924 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 662/1000 - Loss: 0.0889 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 663/1000 - Loss: 0.0861 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 664/1000 - Loss: 0.0791 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 665/1000 - Loss: 0.0983 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 666/1000 - Loss: 0.1010 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 667/1000 - Loss: 0.0976 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 668/1000 - Loss: 0.0806 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 669/1000 - Loss: 0.1104 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 670/1000 - Loss: 0.1158 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 671/1000 - Loss: 0.0908 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 672/1000 - Loss: 0.0996 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 673/1000 - Loss: 0.1165 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 674/1000 - Loss: 0.0911 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 675/1000 - Loss: 0.1073 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 676/1000 - Loss: 0.0772 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 677/1000 - Loss: 0.0795 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 678/1000 - Loss: 0.1035 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 679/1000 - Loss: 0.0896 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 680/1000 - Loss: 0.0933 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 681/1000 - Loss: 0.0959 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 682/1000 - Loss: 0.1144 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 683/1000 - Loss: 0.0872 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 684/1000 - Loss: 0.0954 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 685/1000 - Loss: 0.0846 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 686/1000 - Loss: 0.1036 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 687/1000 - Loss: 0.0888 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 688/1000 - Loss: 0.0886 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 689/1000 - Loss: 0.0889 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 690/1000 - Loss: 0.0883 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 691/1000 - Loss: 0.1005 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 692/1000 - Loss: 0.0886 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 693/1000 - Loss: 0.1069 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 694/1000 - Loss: 0.1148 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 695/1000 - Loss: 0.0943 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 696/1000 - Loss: 0.0860 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 697/1000 - Loss: 0.1173 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 698/1000 - Loss: 0.0931 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 699/1000 - Loss: 0.1041 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 700/1000 - Loss: 0.1174 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 701/1000 - Loss: 0.0964 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 702/1000 - Loss: 0.1013 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 703/1000 - Loss: 0.0988 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 704/1000 - Loss: 0.0949 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 705/1000 - Loss: 0.0989 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 706/1000 - Loss: 0.0900 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 707/1000 - Loss: 0.0818 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 708/1000 - Loss: 0.0900 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 709/1000 - Loss: 0.0873 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 710/1000 - Loss: 0.0889 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 711/1000 - Loss: 0.0785 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 712/1000 - Loss: 0.0871 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 713/1000 - Loss: 0.0753 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 714/1000 - Loss: 0.0955 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 715/1000 - Loss: 0.0943 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 716/1000 - Loss: 0.0876 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 717/1000 - Loss: 0.0961 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 718/1000 - Loss: 0.0928 - F1: 0.7671 - Acc: 0.9150 - Precision: 0.8485 - Recall: 0.7000 - MCC: 0.7207\nEpoch 719/1000 - Loss: 0.0841 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 720/1000 - Loss: 0.0893 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 721/1000 - Loss: 0.1131 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 722/1000 - Loss: 0.0965 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 723/1000 - Loss: 0.0927 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 724/1000 - Loss: 0.1091 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 725/1000 - Loss: 0.0847 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 726/1000 - Loss: 0.0745 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 727/1000 - Loss: 0.0933 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 728/1000 - Loss: 0.0873 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 729/1000 - Loss: 0.1028 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 730/1000 - Loss: 0.0814 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 731/1000 - Loss: 0.0949 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 732/1000 - Loss: 0.0852 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 733/1000 - Loss: 0.0809 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 734/1000 - Loss: 0.1017 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 735/1000 - Loss: 0.0971 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 736/1000 - Loss: 0.0722 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 737/1000 - Loss: 0.1053 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 738/1000 - Loss: 0.0858 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 739/1000 - Loss: 0.0903 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 740/1000 - Loss: 0.1052 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 741/1000 - Loss: 0.1032 - F1: 0.7606 - Acc: 0.9150 - Precision: 0.8710 - Recall: 0.6750 - MCC: 0.7184\nEpoch 742/1000 - Loss: 0.0752 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 743/1000 - Loss: 0.0704 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 744/1000 - Loss: 0.0998 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 745/1000 - Loss: 0.0957 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 746/1000 - Loss: 0.0933 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 747/1000 - Loss: 0.0908 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 748/1000 - Loss: 0.0765 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 749/1000 - Loss: 0.0861 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 750/1000 - Loss: 0.0948 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 751/1000 - Loss: 0.0981 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 752/1000 - Loss: 0.0786 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 753/1000 - Loss: 0.0999 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 754/1000 - Loss: 0.0858 - F1: 0.7027 - Acc: 0.8900 - Precision: 0.7647 - Recall: 0.6500 - MCC: 0.6389\nEpoch 755/1000 - Loss: 0.0760 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 756/1000 - Loss: 0.0863 - F1: 0.7123 - Acc: 0.8950 - Precision: 0.7879 - Recall: 0.6500 - MCC: 0.6533\nEpoch 757/1000 - Loss: 0.0897 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 758/1000 - Loss: 0.0800 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 759/1000 - Loss: 0.0778 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 760/1000 - Loss: 0.0830 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 761/1000 - Loss: 0.0781 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 762/1000 - Loss: 0.0683 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 763/1000 - Loss: 0.0979 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 764/1000 - Loss: 0.0991 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 765/1000 - Loss: 0.0819 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 766/1000 - Loss: 0.0898 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 767/1000 - Loss: 0.1012 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 768/1000 - Loss: 0.0961 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 769/1000 - Loss: 0.1016 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 770/1000 - Loss: 0.0865 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 771/1000 - Loss: 0.0981 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 772/1000 - Loss: 0.1024 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 773/1000 - Loss: 0.0990 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 774/1000 - Loss: 0.0872 - F1: 0.7714 - Acc: 0.9200 - Precision: 0.9000 - Recall: 0.6750 - MCC: 0.7351\nEpoch 775/1000 - Loss: 0.0949 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 776/1000 - Loss: 0.0599 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 777/1000 - Loss: 0.0812 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 778/1000 - Loss: 0.0960 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 779/1000 - Loss: 0.0769 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 780/1000 - Loss: 0.0913 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 781/1000 - Loss: 0.1011 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 782/1000 - Loss: 0.1077 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 783/1000 - Loss: 0.0939 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 784/1000 - Loss: 0.0804 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 785/1000 - Loss: 0.0862 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 786/1000 - Loss: 0.0788 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 787/1000 - Loss: 0.0769 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 788/1000 - Loss: 0.0969 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 789/1000 - Loss: 0.0863 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 790/1000 - Loss: 0.0768 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 791/1000 - Loss: 0.0836 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 792/1000 - Loss: 0.1064 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 793/1000 - Loss: 0.1088 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 794/1000 - Loss: 0.0920 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 795/1000 - Loss: 0.1036 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 796/1000 - Loss: 0.0808 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 797/1000 - Loss: 0.0867 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 798/1000 - Loss: 0.0805 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 799/1000 - Loss: 0.0747 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 800/1000 - Loss: 0.0824 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 801/1000 - Loss: 0.0777 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 802/1000 - Loss: 0.0988 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 803/1000 - Loss: 0.0863 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 804/1000 - Loss: 0.0942 - F1: 0.7123 - Acc: 0.8950 - Precision: 0.7879 - Recall: 0.6500 - MCC: 0.6533\nEpoch 805/1000 - Loss: 0.0972 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 806/1000 - Loss: 0.0905 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 807/1000 - Loss: 0.0981 - F1: 0.6761 - Acc: 0.8850 - Precision: 0.7742 - Recall: 0.6000 - MCC: 0.6148\nEpoch 808/1000 - Loss: 0.0851 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 809/1000 - Loss: 0.0925 - F1: 0.6761 - Acc: 0.8850 - Precision: 0.7742 - Recall: 0.6000 - MCC: 0.6148\nEpoch 810/1000 - Loss: 0.0898 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 811/1000 - Loss: 0.0966 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 812/1000 - Loss: 0.0768 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 813/1000 - Loss: 0.0854 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 814/1000 - Loss: 0.0977 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 815/1000 - Loss: 0.1030 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 816/1000 - Loss: 0.0952 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 817/1000 - Loss: 0.0893 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 818/1000 - Loss: 0.0863 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 819/1000 - Loss: 0.1011 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 820/1000 - Loss: 0.0990 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 821/1000 - Loss: 0.0933 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 822/1000 - Loss: 0.1076 - F1: 0.6567 - Acc: 0.8850 - Precision: 0.8148 - Recall: 0.5500 - MCC: 0.6072\nEpoch 823/1000 - Loss: 0.0922 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 824/1000 - Loss: 0.0980 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 825/1000 - Loss: 0.0845 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 826/1000 - Loss: 0.1125 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 827/1000 - Loss: 0.1220 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 828/1000 - Loss: 0.1039 - F1: 0.6774 - Acc: 0.9000 - Precision: 0.9545 - Recall: 0.5250 - MCC: 0.6632\nEpoch 829/1000 - Loss: 0.0933 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 830/1000 - Loss: 0.0951 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 831/1000 - Loss: 0.0914 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 832/1000 - Loss: 0.0956 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 833/1000 - Loss: 0.0747 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 834/1000 - Loss: 0.1066 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 835/1000 - Loss: 0.0763 - F1: 0.6667 - Acc: 0.8950 - Precision: 0.9130 - Recall: 0.5250 - MCC: 0.6426\nEpoch 836/1000 - Loss: 0.0918 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 837/1000 - Loss: 0.1009 - F1: 0.7273 - Acc: 0.8950 - Precision: 0.7568 - Recall: 0.7000 - MCC: 0.6632\nEpoch 838/1000 - Loss: 0.0797 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 839/1000 - Loss: 0.1064 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\nEpoch 840/1000 - Loss: 0.0830 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 841/1000 - Loss: 0.1096 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 842/1000 - Loss: 0.0990 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 843/1000 - Loss: 0.0823 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 844/1000 - Loss: 0.0929 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 845/1000 - Loss: 0.0863 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 846/1000 - Loss: 0.0950 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 847/1000 - Loss: 0.0767 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 848/1000 - Loss: 0.0972 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 849/1000 - Loss: 0.0895 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 850/1000 - Loss: 0.0954 - F1: 0.7397 - Acc: 0.9050 - Precision: 0.8182 - Recall: 0.6750 - MCC: 0.6870\nEpoch 851/1000 - Loss: 0.0683 - F1: 0.7297 - Acc: 0.9000 - Precision: 0.7941 - Recall: 0.6750 - MCC: 0.6722\nEpoch 852/1000 - Loss: 0.0747 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 853/1000 - Loss: 0.1226 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 854/1000 - Loss: 0.0740 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 855/1000 - Loss: 0.0878 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 856/1000 - Loss: 0.0917 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 857/1000 - Loss: 0.0867 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 858/1000 - Loss: 0.0860 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 859/1000 - Loss: 0.0774 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 860/1000 - Loss: 0.0982 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 861/1000 - Loss: 0.1062 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 862/1000 - Loss: 0.0781 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 863/1000 - Loss: 0.0980 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 864/1000 - Loss: 0.0866 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 865/1000 - Loss: 0.0819 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 866/1000 - Loss: 0.0854 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 867/1000 - Loss: 0.0848 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 868/1000 - Loss: 0.0875 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 869/1000 - Loss: 0.0962 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 870/1000 - Loss: 0.0797 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 871/1000 - Loss: 0.0829 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 872/1000 - Loss: 0.0913 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 873/1000 - Loss: 0.0844 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 874/1000 - Loss: 0.0856 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 875/1000 - Loss: 0.0793 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 876/1000 - Loss: 0.0964 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 877/1000 - Loss: 0.1015 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 878/1000 - Loss: 0.1189 - F1: 0.7671 - Acc: 0.9150 - Precision: 0.8485 - Recall: 0.7000 - MCC: 0.7207\nEpoch 879/1000 - Loss: 0.0764 - F1: 0.7778 - Acc: 0.9200 - Precision: 0.8750 - Recall: 0.7000 - MCC: 0.7365\nEpoch 880/1000 - Loss: 0.1110 - F1: 0.7671 - Acc: 0.9150 - Precision: 0.8485 - Recall: 0.7000 - MCC: 0.7207\nEpoch 881/1000 - Loss: 0.1120 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 882/1000 - Loss: 0.0809 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 883/1000 - Loss: 0.0746 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 884/1000 - Loss: 0.0838 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 885/1000 - Loss: 0.0831 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 886/1000 - Loss: 0.0725 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 887/1000 - Loss: 0.0894 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 888/1000 - Loss: 0.0889 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 889/1000 - Loss: 0.1028 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 890/1000 - Loss: 0.0781 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 891/1000 - Loss: 0.0798 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 892/1000 - Loss: 0.1080 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 893/1000 - Loss: 0.0833 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 894/1000 - Loss: 0.1185 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 895/1000 - Loss: 0.0939 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 896/1000 - Loss: 0.1024 - F1: 0.6765 - Acc: 0.8900 - Precision: 0.8214 - Recall: 0.5750 - MCC: 0.6268\nEpoch 897/1000 - Loss: 0.1146 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 898/1000 - Loss: 0.0951 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 899/1000 - Loss: 0.1017 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 900/1000 - Loss: 0.0840 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 901/1000 - Loss: 0.0870 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 902/1000 - Loss: 0.0857 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 903/1000 - Loss: 0.0913 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 904/1000 - Loss: 0.0656 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 905/1000 - Loss: 0.0949 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 906/1000 - Loss: 0.0922 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 907/1000 - Loss: 0.0831 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 908/1000 - Loss: 0.0846 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 909/1000 - Loss: 0.0996 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 910/1000 - Loss: 0.0901 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 911/1000 - Loss: 0.0983 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 912/1000 - Loss: 0.0866 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 913/1000 - Loss: 0.0879 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 914/1000 - Loss: 0.0980 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 915/1000 - Loss: 0.0913 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 916/1000 - Loss: 0.0848 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 917/1000 - Loss: 0.0881 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 918/1000 - Loss: 0.0954 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 919/1000 - Loss: 0.0862 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 920/1000 - Loss: 0.0917 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 921/1000 - Loss: 0.0761 - F1: 0.7188 - Acc: 0.9100 - Precision: 0.9583 - Recall: 0.5750 - MCC: 0.7001\nEpoch 922/1000 - Loss: 0.0816 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 923/1000 - Loss: 0.0851 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 924/1000 - Loss: 0.1092 - F1: 0.6944 - Acc: 0.8900 - Precision: 0.7812 - Recall: 0.6250 - MCC: 0.6342\nEpoch 925/1000 - Loss: 0.0702 - F1: 0.6857 - Acc: 0.8900 - Precision: 0.8000 - Recall: 0.6000 - MCC: 0.6301\nEpoch 926/1000 - Loss: 0.0855 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 927/1000 - Loss: 0.0894 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 928/1000 - Loss: 0.1142 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 929/1000 - Loss: 0.0835 - F1: 0.6866 - Acc: 0.8950 - Precision: 0.8519 - Recall: 0.5750 - MCC: 0.6438\nEpoch 930/1000 - Loss: 0.1267 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 931/1000 - Loss: 0.0928 - F1: 0.6667 - Acc: 0.8900 - Precision: 0.8462 - Recall: 0.5500 - MCC: 0.6244\nEpoch 932/1000 - Loss: 0.1027 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 933/1000 - Loss: 0.0732 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 934/1000 - Loss: 0.0808 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 935/1000 - Loss: 0.0871 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 936/1000 - Loss: 0.0778 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 937/1000 - Loss: 0.0903 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 938/1000 - Loss: 0.1213 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 939/1000 - Loss: 0.0841 - F1: 0.7576 - Acc: 0.9200 - Precision: 0.9615 - Recall: 0.6250 - MCC: 0.7359\nEpoch 940/1000 - Loss: 0.1188 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 941/1000 - Loss: 0.0827 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 942/1000 - Loss: 0.0744 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 943/1000 - Loss: 0.0812 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 944/1000 - Loss: 0.0799 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 945/1000 - Loss: 0.0749 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 946/1000 - Loss: 0.0974 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 947/1000 - Loss: 0.0912 - F1: 0.6563 - Acc: 0.8900 - Precision: 0.8750 - Recall: 0.5250 - MCC: 0.6232\nEpoch 948/1000 - Loss: 0.1141 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 949/1000 - Loss: 0.0869 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 950/1000 - Loss: 0.0747 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 951/1000 - Loss: 0.1043 - F1: 0.6452 - Acc: 0.8900 - Precision: 0.9091 - Recall: 0.5000 - MCC: 0.6232\nEpoch 952/1000 - Loss: 0.0760 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 953/1000 - Loss: 0.0789 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 954/1000 - Loss: 0.0803 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 955/1000 - Loss: 0.0744 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 956/1000 - Loss: 0.0836 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 957/1000 - Loss: 0.0614 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 958/1000 - Loss: 0.0679 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 959/1000 - Loss: 0.0746 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 960/1000 - Loss: 0.0662 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 961/1000 - Loss: 0.0784 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 962/1000 - Loss: 0.0771 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 963/1000 - Loss: 0.0724 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 964/1000 - Loss: 0.0844 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 965/1000 - Loss: 0.0820 - F1: 0.6769 - Acc: 0.8950 - Precision: 0.8800 - Recall: 0.5500 - MCC: 0.6425\nEpoch 966/1000 - Loss: 0.1062 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 967/1000 - Loss: 0.0855 - F1: 0.6875 - Acc: 0.9000 - Precision: 0.9167 - Recall: 0.5500 - MCC: 0.6616\nEpoch 968/1000 - Loss: 0.0832 - F1: 0.7324 - Acc: 0.9050 - Precision: 0.8387 - Recall: 0.6500 - MCC: 0.6839\nEpoch 969/1000 - Loss: 0.0735 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 970/1000 - Loss: 0.0858 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 971/1000 - Loss: 0.0970 - F1: 0.7568 - Acc: 0.9100 - Precision: 0.8235 - Recall: 0.7000 - MCC: 0.7055\nEpoch 972/1000 - Loss: 0.0955 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 973/1000 - Loss: 0.0965 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 974/1000 - Loss: 0.0912 - F1: 0.7222 - Acc: 0.9000 - Precision: 0.8125 - Recall: 0.6500 - MCC: 0.6683\nEpoch 975/1000 - Loss: 0.1054 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nEpoch 976/1000 - Loss: 0.0836 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 977/1000 - Loss: 0.0973 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 978/1000 - Loss: 0.0752 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 979/1000 - Loss: 0.0928 - F1: 0.7077 - Acc: 0.9050 - Precision: 0.9200 - Recall: 0.5750 - MCC: 0.6803\nEpoch 980/1000 - Loss: 0.0865 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 981/1000 - Loss: 0.0907 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 982/1000 - Loss: 0.0872 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 983/1000 - Loss: 0.0755 - F1: 0.7273 - Acc: 0.9100 - Precision: 0.9231 - Recall: 0.6000 - MCC: 0.6988\nEpoch 984/1000 - Loss: 0.0805 - F1: 0.7164 - Acc: 0.9050 - Precision: 0.8889 - Recall: 0.6000 - MCC: 0.6804\nEpoch 985/1000 - Loss: 0.0844 - F1: 0.7536 - Acc: 0.9150 - Precision: 0.8966 - Recall: 0.6500 - MCC: 0.7171\nEpoch 986/1000 - Loss: 0.1013 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 987/1000 - Loss: 0.0855 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 988/1000 - Loss: 0.0744 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 989/1000 - Loss: 0.0890 - F1: 0.7059 - Acc: 0.9000 - Precision: 0.8571 - Recall: 0.6000 - MCC: 0.6628\nEpoch 990/1000 - Loss: 0.1097 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 991/1000 - Loss: 0.0763 - F1: 0.7042 - Acc: 0.8950 - Precision: 0.8065 - Recall: 0.6250 - MCC: 0.6493\nEpoch 992/1000 - Loss: 0.0840 - F1: 0.7246 - Acc: 0.9050 - Precision: 0.8621 - Recall: 0.6250 - MCC: 0.6816\nEpoch 993/1000 - Loss: 0.0775 - F1: 0.7353 - Acc: 0.9100 - Precision: 0.8929 - Recall: 0.6250 - MCC: 0.6989\nEpoch 994/1000 - Loss: 0.0831 - F1: 0.7429 - Acc: 0.9100 - Precision: 0.8667 - Recall: 0.6500 - MCC: 0.7001\nEpoch 995/1000 - Loss: 0.0918 - F1: 0.7467 - Acc: 0.9050 - Precision: 0.8000 - Recall: 0.7000 - MCC: 0.6908\nEpoch 996/1000 - Loss: 0.0908 - F1: 0.7714 - Acc: 0.9200 - Precision: 0.9000 - Recall: 0.6750 - MCC: 0.7351\nEpoch 997/1000 - Loss: 0.0831 - F1: 0.7500 - Acc: 0.9100 - Precision: 0.8438 - Recall: 0.6750 - MCC: 0.7024\nEpoch 998/1000 - Loss: 0.1044 - F1: 0.6970 - Acc: 0.9000 - Precision: 0.8846 - Recall: 0.5750 - MCC: 0.6616\nEpoch 999/1000 - Loss: 0.1094 - F1: 0.6957 - Acc: 0.8950 - Precision: 0.8276 - Recall: 0.6000 - MCC: 0.6461\nEpoch 1000/1000 - Loss: 0.0849 - F1: 0.7143 - Acc: 0.9000 - Precision: 0.8333 - Recall: 0.6250 - MCC: 0.6651\nLoaded best model from /kaggle/working/metrics_model_best_f1.pth\n\nFinal test metrics:\nAccuracy: 0.9200\nPrecision: 0.8333\nRecall: 0.7500\nF1-Score: 0.7895\nMCC: 0.7418\nROC-AUC: 0.9467\nConfusion Matrix:\n [[154   6]\n [ 10  30]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0     0.9390    0.9625    0.9506       160\n           1     0.8333    0.7500    0.7895        40\n\n    accuracy                         0.9200       200\n   macro avg     0.8862    0.8562    0.8700       200\nweighted avg     0.9179    0.9200    0.9184       200\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    matthews_corrcoef, confusion_matrix, classification_report,\n    roc_auc_score, roc_curve\n)\nimport matplotlib.pyplot as plt\nimport random\n\n# ---------------------------\n# Config\n# ---------------------------\nSEED = 42\nBATCH_SIZE = 64\nEPOCHS = 1000\nLEARNING_RATE = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nDATA_PATH = \"/kaggle/input/python-dataset/Python_LongMethodSmell_Dataset.csv\"\nMODEL_OUT = \"metrics_model_best_f1.pth\"\nDO_CV = False\n\n# ---------------------------\n# Reproducibility\n# ---------------------------\ndef set_seed(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# ---------------------------\n# Dataset Loader\n# ---------------------------\nclass MetricsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\nclass DataLoaderWrapper:\n    def __init__(self, path):\n        self.path = path\n        self.df = None\n\n    def load(self):\n        self.df = pd.read_csv(self.path)\n        self.df.columns = self.df.columns.str.strip()\n        return self.df\n\n    def preprocess(self):\n        if self.df is None:\n            self.load()\n\n        if \"Experince Based\" not in self.df.columns:\n            raise ValueError(f\"Target column not found. Available: {self.df.columns.tolist()}\")\n\n        X = self.df.drop(columns=[\"Experince Based\"]).values.astype(float)\n        y = self.df[\"Experince Based\"].astype(int).values\n        return X, y\n\n# ---------------------------\n# Neural Net Model\n# ---------------------------\nclass MetricsModel(nn.Module):\n    def __init__(self, input_dim):\n        super(MetricsModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n# ---------------------------\n# Training\n# ---------------------------\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS):\n    best_f1 = 0\n    best_state = None\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE).unsqueeze(1)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_preds, val_targets = [], []\n        with torch.no_grad():\n            for X_val, y_val in val_loader:\n                X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE).unsqueeze(1)\n                preds = model(X_val)\n                val_preds.extend(preds.cpu().numpy())\n                val_targets.extend(y_val.cpu().numpy())\n\n        val_preds_bin = (np.array(val_preds) > 0.5).astype(int)\n\n        acc = accuracy_score(val_targets, val_preds_bin)\n        prec = precision_score(val_targets, val_preds_bin, zero_division=0)\n        rec = recall_score(val_targets, val_preds_bin, zero_division=0)\n        f1 = f1_score(val_targets, val_preds_bin, zero_division=0)\n        mcc = matthews_corrcoef(val_targets, val_preds_bin)\n        cm = confusion_matrix(val_targets, val_preds_bin)\n        class_report = classification_report(val_targets, val_preds_bin, zero_division=0)\n        try:\n            roc_auc = roc_auc_score(val_targets, val_preds)\n        except:\n            roc_auc = float(\"nan\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_state = model.state_dict()\n\n        if epoch % 50 == 0 or epoch == epochs:\n            print(\n                f\"Epoch {epoch}/{epochs} - Loss: {np.mean(train_losses):.4f} - \"\n                f\"Acc: {acc:.4f} - Prec: {prec:.4f} - Rec: {rec:.4f} - F1: {f1:.4f} - MCC: {mcc:.4f} - ROC-AUC: {roc_auc:.4f}\"\n            )\n\n    model.load_state_dict(best_state)\n\n    # Final evaluation\n    final_preds, final_targets = [], []\n    with torch.no_grad():\n        for X_val, y_val in val_loader:\n            X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE).unsqueeze(1)\n            preds = model(X_val)\n            final_preds.extend(preds.cpu().numpy())\n            final_targets.extend(y_val.cpu().numpy())\n\n    final_preds_bin = (np.array(final_preds) > 0.5).astype(int)\n    final_metrics = {\n        \"accuracy\": accuracy_score(final_targets, final_preds_bin),\n        \"precision\": precision_score(final_targets, final_preds_bin, zero_division=0),\n        \"recall\": recall_score(final_targets, final_preds_bin, zero_division=0),\n        \"f1\": f1_score(final_targets, final_preds_bin, zero_division=0),\n        \"mcc\": matthews_corrcoef(final_targets, final_preds_bin),\n        \"confusion_matrix\": confusion_matrix(final_targets, final_preds_bin),\n        \"classification_report\": classification_report(final_targets, final_preds_bin, zero_division=0),\n    }\n    try:\n        final_metrics[\"roc_auc\"] = roc_auc_score(final_targets, final_preds)\n        fpr, tpr, _ = roc_curve(final_targets, final_preds)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {final_metrics['roc_auc']:.4f})\")\n        plt.plot([0, 1], [0, 1], \"k--\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.title(\"ROC Curve\")\n        plt.legend()\n        plt.show()\n    except:\n        final_metrics[\"roc_auc\"] = float(\"nan\")\n\n    return model, final_metrics\n\n# ---------------------------\n# Main\n# ---------------------------\ndef main():\n    loader = DataLoaderWrapper(DATA_PATH)\n    df = loader.load()\n    print(f\"Loaded dataset with {len(df)} rows and columns: {df.columns.tolist()}\")\n\n    X, y = loader.preprocess()\n    print(f\"Features shape: {X.shape}, Labels shape: {y.shape}\")\n    print(\"Overall class distribution:\", np.bincount(y))\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n\n    train_dataset = MetricsDataset(X_train, y_train)\n    val_dataset = MetricsDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n    model = MetricsModel(input_dim=X.shape[1]).to(DEVICE)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    model, final_metrics = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)\n\n    torch.save(model.state_dict(), MODEL_OUT)\n    print(f\"Best model saved to {MODEL_OUT}\")\n\n    print(\"\\n=== Final Evaluation Metrics ===\")\n    for k, v in final_metrics.items():\n        if k in [\"confusion_matrix\", \"classification_report\"]:\n            print(f\"{k}:\\n{v}\")\n        else:\n            print(f\"{k}: {v:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:35:20.495944Z","iopub.execute_input":"2025-10-01T05:35:20.496318Z","iopub.status.idle":"2025-10-01T05:36:05.116617Z","shell.execute_reply.started":"2025-10-01T05:35:20.496300Z","shell.execute_reply":"2025-10-01T05:36:05.116011Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset with 894 rows and columns: ['loc', 'lloc', 'scloc', 'comments', 'single_comments', 'multi_comments', 'blanks', 'h1', 'h2', 'n1', 'n2', 'vocabulary', 'length', 'calculated_length', 'volume', 'difficulty', 'effort', 'time', 'bugs', 'Experince Based']\nFeatures shape: (894, 19), Labels shape: (894,)\nOverall class distribution: [685 209]\nEpoch 50/1000 - Loss: 0.3513 - Acc: 0.8380 - Prec: 0.7600 - Rec: 0.4524 - F1: 0.5672 - MCC: 0.4995 - ROC-AUC: 0.9301\nEpoch 100/1000 - Loss: 0.2752 - Acc: 0.8101 - Prec: 0.8333 - Rec: 0.2381 - F1: 0.3704 - MCC: 0.3787 - ROC-AUC: 0.9559\nEpoch 150/1000 - Loss: 0.3101 - Acc: 0.9274 - Prec: 0.7736 - Rec: 0.9762 - F1: 0.8632 - MCC: 0.8248 - ROC-AUC: 0.9750\nEpoch 200/1000 - Loss: 0.2120 - Acc: 0.9330 - Prec: 0.7885 - Rec: 0.9762 - F1: 0.8723 - MCC: 0.8363 - ROC-AUC: 0.9765\nEpoch 250/1000 - Loss: 0.1779 - Acc: 0.9385 - Prec: 0.8298 - Rec: 0.9286 - F1: 0.8764 - MCC: 0.8380 - ROC-AUC: 0.9743\nEpoch 300/1000 - Loss: 0.2555 - Acc: 0.9497 - Prec: 0.8511 - Rec: 0.9524 - F1: 0.8989 - MCC: 0.8680 - ROC-AUC: 0.9805\nEpoch 350/1000 - Loss: 0.2684 - Acc: 0.9330 - Prec: 0.7885 - Rec: 0.9762 - F1: 0.8723 - MCC: 0.8363 - ROC-AUC: 0.9755\nEpoch 400/1000 - Loss: 0.2170 - Acc: 0.9497 - Prec: 0.8511 - Rec: 0.9524 - F1: 0.8989 - MCC: 0.8680 - ROC-AUC: 0.9780\nEpoch 450/1000 - Loss: 0.1357 - Acc: 0.9385 - Prec: 0.7925 - Rec: 1.0000 - F1: 0.8842 - MCC: 0.8537 - ROC-AUC: 0.9786\nEpoch 500/1000 - Loss: 0.1286 - Acc: 0.9330 - Prec: 0.8000 - Rec: 0.9524 - F1: 0.8696 - MCC: 0.8306 - ROC-AUC: 0.9718\nEpoch 550/1000 - Loss: 0.1144 - Acc: 0.9330 - Prec: 0.8261 - Rec: 0.9048 - F1: 0.8636 - MCC: 0.8208 - ROC-AUC: 0.9730\nEpoch 600/1000 - Loss: 0.0973 - Acc: 0.9385 - Prec: 0.8163 - Rec: 0.9524 - F1: 0.8791 - MCC: 0.8427 - ROC-AUC: 0.9732\nEpoch 650/1000 - Loss: 0.1022 - Acc: 0.9497 - Prec: 0.8367 - Rec: 0.9762 - F1: 0.9011 - MCC: 0.8723 - ROC-AUC: 0.9791\nEpoch 700/1000 - Loss: 0.1363 - Acc: 0.9330 - Prec: 0.7885 - Rec: 0.9762 - F1: 0.8723 - MCC: 0.8363 - ROC-AUC: 0.9787\nEpoch 750/1000 - Loss: 0.0912 - Acc: 0.9497 - Prec: 0.8367 - Rec: 0.9762 - F1: 0.9011 - MCC: 0.8723 - ROC-AUC: 0.9718\nEpoch 800/1000 - Loss: 0.0906 - Acc: 0.9441 - Prec: 0.8333 - Rec: 0.9524 - F1: 0.8889 - MCC: 0.8552 - ROC-AUC: 0.9787\nEpoch 850/1000 - Loss: 0.0734 - Acc: 0.9441 - Prec: 0.8200 - Rec: 0.9762 - F1: 0.8913 - MCC: 0.8600 - ROC-AUC: 0.9720\nEpoch 900/1000 - Loss: 0.0931 - Acc: 0.9441 - Prec: 0.8200 - Rec: 0.9762 - F1: 0.8913 - MCC: 0.8600 - ROC-AUC: 0.9768\nEpoch 950/1000 - Loss: 0.1184 - Acc: 0.9441 - Prec: 0.8200 - Rec: 0.9762 - F1: 0.8913 - MCC: 0.8600 - ROC-AUC: 0.9728\nEpoch 1000/1000 - Loss: 0.0791 - Acc: 0.9497 - Prec: 0.8367 - Rec: 0.9762 - F1: 0.9011 - MCC: 0.8723 - ROC-AUC: 0.9801\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Best model saved to metrics_model_best_f1.pth\n\n=== Final Evaluation Metrics ===\naccuracy: 0.9497\nprecision: 0.8367\nrecall: 0.9762\nf1: 0.9011\nmcc: 0.8723\nconfusion_matrix:\n[[129   8]\n [  1  41]]\nclassification_report:\n              precision    recall  f1-score   support\n\n         0.0       0.99      0.94      0.97       137\n         1.0       0.84      0.98      0.90        42\n\n    accuracy                           0.95       179\n   macro avg       0.91      0.96      0.93       179\nweighted avg       0.96      0.95      0.95       179\n\nroc_auc: 0.9801\n","output_type":"stream"}],"execution_count":2}]}